"""
services/utils/vulnerability_deduplicator.py
Smart vulnerability deduplication using ChromaDB RAG for semantic similarity
"""
import os
import json
import hashlib
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from pathlib import Path

try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    logging.warning("ChromaDB not installed. Run: pip install chromadb")

logger = logging.getLogger(__name__)


class VulnerabilityDeduplicator:
    """
    Smart deduplication using ChromaDB semantic similarity.

    Workflow:
    1. For each finding, create embedding text (normalized_signature)
    2. Query ChromaDB for similar findings (similarity > 0.90)
    3. If similar finding exists, merge locations
    4. Otherwise, store as new unique finding
    """

    def __init__(
        self,
        job_id: str,
        chroma_persist_dir: str = "./chroma_db",
        similarity_threshold: float = 0.90,
        record_dir: str = "./reports"
    ):
        """
        Initialize Vulnerability Deduplicator.

        Args:
            job_id: Unique job identifier
            chroma_persist_dir: ChromaDB persistence directory
            similarity_threshold: Cosine similarity threshold (0.90 = strict)
            record_dir: Directory to save deduplication records
        """
        if not CHROMADB_AVAILABLE:
            raise ImportError("ChromaDB not installed. Run: pip install chromadb")

        self.job_id = job_id
        self.similarity_threshold = similarity_threshold
        self.record_dir = Path(record_dir) / str(job_id) / "deduplication"
        self.record_dir.mkdir(parents=True, exist_ok=True)

        # Initialize ChromaDB
        self.chroma_client = chromadb.PersistentClient(
            path=chroma_persist_dir,
            settings=Settings(anonymized_telemetry=False)
        )

        # Collection for vulnerability findings
        collection_name = f"vuln_dedup_{job_id}"
        try:
            # Delete existing collection for this job (fresh start)
            self.chroma_client.delete_collection(name=collection_name)
            logger.info(f"Deleted existing collection: {collection_name}")
        except:
            pass

        self.vuln_collection = self.chroma_client.get_or_create_collection(
            name=collection_name,
            metadata={"description": f"Vulnerability deduplication for job {job_id}"}
        )

        # In-memory registry for quick access
        self.vuln_registry: Dict[str, Dict[str, Any]] = {}
        self.stats = {
            'total_processed': 0,
            'unique_findings': 0,
            'duplicates_merged': 0,
            'exact_signature_matches': 0,
            'semantic_matches': 0
        }

        logger.info(f"VulnerabilityDeduplicator initialized for job {job_id}")
        logger.info(f"  Similarity threshold: {similarity_threshold}")

    def _generate_vuln_id(self) -> str:
        """Generate unique vulnerability ID (8 chars)."""
        import uuid
        return str(uuid.uuid4())[:8]

    def _create_embedding_text(self, finding: Dict[str, Any]) -> str:
        """
        Create text representation for embedding generation.
        Format: "VulnType:PrimaryIndicator"

        Example: "XSS:/search?q=" or "SQLi:/api/users"
        """
        vuln_type = finding.get('vuln_type', 'Unknown')
        primary_indicator = finding.get('primary_indicator', '')
        vector = finding.get('vector', '')

        # Build normalized signature
        if primary_indicator:
            signature = f"{vuln_type}:{primary_indicator}"
        else:
            # Fallback: use vector
            signature = f"{vuln_type}:{vector}"

        return signature

    def _create_normalized_signature(self, finding: Dict[str, Any]) -> str:
        """
        Create normalized signature for exact matching.
        This is deterministic and used for Stage 1 deduplication.
        """
        vuln_type = finding.get('vuln_type', 'Unknown').strip().lower()
        primary_indicator = finding.get('primary_indicator', '').strip().lower()

        # Normalize URL paths (remove trailing slashes, query params)
        if primary_indicator:
            # Remove query params for more aggressive grouping
            if '?' in primary_indicator:
                primary_indicator = primary_indicator.split('?')[0]
            primary_indicator = primary_indicator.rstrip('/')

        return f"{vuln_type}:{primary_indicator}"

    def process_finding(
        self,
        finding: Dict[str, Any],
        url: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Process a single finding through deduplication pipeline.

        Args:
            finding: Raw finding from web analysis
            url: URL where finding was discovered
            metadata: Optional metadata (page_type, screenshot_path, etc.)

        Returns:
            Result dict with action taken (new/merged) and vuln_id
        """
        self.stats['total_processed'] += 1

        # Extract key fields
        vuln_type = finding.get('vuln_type', 'Unknown')
        vector = finding.get('vector', '')
        primary_indicator = finding.get('primary_indicator', '')
        evidence = finding.get('evidence', {})

        # Create signatures
        embedding_text = self._create_embedding_text(finding)
        normalized_signature = self._create_normalized_signature(finding)

        logger.debug(f"Processing finding: {normalized_signature}")

        # Stage 1: Exact signature match (fast path)
        exact_match = self._find_by_exact_signature(normalized_signature)
        if exact_match:
            vuln_id = exact_match['vuln_id']
            self._merge_finding(vuln_id, url, evidence, metadata)
            self.stats['duplicates_merged'] += 1
            self.stats['exact_signature_matches'] += 1

            logger.info(f"  âœ“ Exact match: {normalized_signature} â†’ {vuln_id}")

            return {
                'action': 'merged',
                'vuln_id': vuln_id,
                'match_type': 'exact_signature',
                'original_finding': finding
            }

        # Stage 2: Semantic similarity with ChromaDB
        similar_findings = self._query_similar_findings(embedding_text, n_results=3)

        if similar_findings:
            # Check if any are above threshold
            for similar in similar_findings:
                similarity = similar.get('similarity', 0.0)
                if similarity >= self.similarity_threshold:
                    vuln_id = similar['vuln_id']
                    self._merge_finding(vuln_id, url, evidence, metadata)
                    self.stats['duplicates_merged'] += 1
                    self.stats['semantic_matches'] += 1

                    logger.info(f"  âœ“ Semantic match: {normalized_signature} â†’ {vuln_id} (sim: {similarity:.3f})")

                    return {
                        'action': 'merged',
                        'vuln_id': vuln_id,
                        'match_type': 'semantic',
                        'similarity': similarity,
                        'original_finding': finding
                    }

        # Stage 3: No match found, create new unique finding
        vuln_id = self._generate_vuln_id()
        self._store_new_finding(vuln_id, finding, url, embedding_text, normalized_signature, metadata)
        self.stats['unique_findings'] += 1

        logger.info(f"  ðŸ†• New finding: {normalized_signature} â†’ {vuln_id}")

        return {
            'action': 'new',
            'vuln_id': vuln_id,
            'match_type': None,
            'original_finding': finding
        }

    def _find_by_exact_signature(self, normalized_signature: str) -> Optional[Dict[str, Any]]:
        """Find existing finding by exact normalized signature (fast lookup)."""
        for vuln_id, vuln_data in self.vuln_registry.items():
            if vuln_data.get('normalized_signature') == normalized_signature:
                return vuln_data
        return None

    def _query_similar_findings(
        self,
        embedding_text: str,
        n_results: int = 3
    ) -> List[Dict[str, Any]]:
        """
        Query ChromaDB for semantically similar findings.

        Returns:
            List of similar findings with similarity scores
        """
        try:
            # Query ChromaDB
            results = self.vuln_collection.query(
                query_texts=[embedding_text],
                n_results=n_results
            )

            similar_findings = []

            if results and results['ids'] and len(results['ids'][0]) > 0:
                for i in range(len(results['ids'][0])):
                    vuln_id = results['ids'][0][i]
                    metadata = results['metadatas'][0][i]
                    distance = results['distances'][0][i] if 'distances' in results else 1.0

                    # Convert distance to similarity (cosine similarity = 1 - distance)
                    similarity = 1.0 - distance

                    vuln_data = json.loads(metadata['vuln_json'])
                    vuln_data['similarity'] = similarity
                    similar_findings.append(vuln_data)

            return similar_findings

        except Exception as e:
            logger.error(f"Failed to query similar findings: {e}")
            return []

    def _merge_finding(
        self,
        vuln_id: str,
        url: str,
        evidence: Dict[str, Any],
        metadata: Optional[Dict[str, Any]]
    ):
        """
        Merge new finding instance into existing vulnerability.
        Updates affected_locations list.
        """
        if vuln_id not in self.vuln_registry:
            logger.error(f"Cannot merge: vuln_id {vuln_id} not in registry")
            return

        existing = self.vuln_registry[vuln_id]

        # Check if this URL already exists
        existing_urls = [loc['url'] for loc in existing.get('affected_locations', [])]
        if url in existing_urls:
            logger.debug(f"  URL already in affected_locations: {url}")
            return

        # Add new location
        if 'affected_locations' not in existing:
            existing['affected_locations'] = []

        existing['affected_locations'].append({
            'url': url,
            'evidence': evidence,
            'page_type': metadata.get('page_type') if metadata else None,
            'screenshot_path': metadata.get('screenshot_path') if metadata else None,
            'discovered_at': datetime.now().isoformat()
        })

        # Update counts
        existing['instance_count'] = len(existing['affected_locations'])
        existing['last_seen'] = datetime.now().isoformat()

        # Update in registry
        self.vuln_registry[vuln_id] = existing

        # Update in ChromaDB
        self._update_chromadb_record(vuln_id, existing)

        logger.debug(f"  Merged into {vuln_id}: now affects {existing['instance_count']} locations")

    def _store_new_finding(
        self,
        vuln_id: str,
        finding: Dict[str, Any],
        url: str,
        embedding_text: str,
        normalized_signature: str,
        metadata: Optional[Dict[str, Any]]
    ):
        """Store new unique finding in ChromaDB and registry."""

        vuln_record = {
            'vuln_id': vuln_id,
            'vuln_type': finding.get('vuln_type', 'Unknown'),
            'vector': finding.get('vector', ''),
            'primary_indicator': finding.get('primary_indicator', ''),
            'normalized_signature': normalized_signature,
            'embedding_text': embedding_text,
            'affected_locations': [{
                'url': url,
                'evidence': finding.get('evidence', {}),
                'page_type': metadata.get('page_type') if metadata else None,
                'screenshot_path': metadata.get('screenshot_path') if metadata else None,
                'discovered_at': datetime.now().isoformat()
            }],
            'payload': finding.get('payload', []),
            'related_endpoints': finding.get('related_endpoints', []),
            'instance_count': 1,
            'first_seen': datetime.now().isoformat(),
            'last_seen': datetime.now().isoformat(),
            'original_finding': finding
        }

        # Store in ChromaDB
        self.vuln_collection.add(
            ids=[vuln_id],
            documents=[embedding_text],
            metadatas=[{
                'vuln_id': vuln_id,
                'vuln_type': vuln_record['vuln_type'],
                'normalized_signature': normalized_signature,
                'first_seen': vuln_record['first_seen'],
                'vuln_json': json.dumps(vuln_record)
            }]
        )

        # Store in registry
        self.vuln_registry[vuln_id] = vuln_record

    def _update_chromadb_record(self, vuln_id: str, vuln_data: Dict[str, Any]):
        """Update existing ChromaDB record (delete and re-add)."""
        try:
            self.vuln_collection.delete(ids=[vuln_id])

            self.vuln_collection.add(
                ids=[vuln_id],
                documents=[vuln_data['embedding_text']],
                metadatas=[{
                    'vuln_id': vuln_id,
                    'vuln_type': vuln_data['vuln_type'],
                    'normalized_signature': vuln_data['normalized_signature'],
                    'first_seen': vuln_data['first_seen'],
                    'vuln_json': json.dumps(vuln_data)
                }]
            )
        except Exception as e:
            logger.error(f"Failed to update ChromaDB record {vuln_id}: {e}")

    def get_deduplicated_findings(self) -> List[Dict[str, Any]]:
        """
        Get all unique findings after deduplication.

        Returns:
            List of unique vulnerability findings sorted by instance_count
        """
        # Sort by instance count (most widespread first)
        sorted_findings = sorted(
            self.vuln_registry.values(),
            key=lambda v: v['instance_count'],
            reverse=True
        )

        return sorted_findings

    def get_statistics(self) -> Dict[str, Any]:
        """Get deduplication statistics."""
        return {
            'total_processed': self.stats['total_processed'],
            'unique_findings': self.stats['unique_findings'],
            'duplicates_merged': self.stats['duplicates_merged'],
            'exact_signature_matches': self.stats['exact_signature_matches'],
            'semantic_matches': self.stats['semantic_matches'],
            'deduplication_ratio': f"{self.stats['total_processed'] / max(self.stats['unique_findings'], 1):.2f}x",
            'unique_finding_ids': list(self.vuln_registry.keys())
        }

    def save_report(self) -> Path:
        """Save deduplication report to JSON file."""
        report = {
            'job_id': self.job_id,
            'generated_at': datetime.now().isoformat(),
            'statistics': self.get_statistics(),
            'unique_findings': self.get_deduplicated_findings()
        }

        output_path = self.record_dir / "deduplication_report.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        logger.info(f"ðŸ’¾ Deduplication report saved: {output_path}")
        return output_path


def deduplicate_findings(
    findings: List[Dict[str, Any]],
    job_id: str,
    similarity_threshold: float = 0.90
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Convenience function to deduplicate a list of findings.

    Args:
        findings: List of raw findings from web analysis
        job_id: Job identifier
        similarity_threshold: Similarity threshold (default: 0.90)

    Returns:
        Tuple of (deduplicated_findings, statistics)
    """
    deduplicator = VulnerabilityDeduplicator(
        job_id=job_id,
        similarity_threshold=similarity_threshold
    )

    for finding in findings:
        url = finding.get('url', 'unknown')
        metadata = {
            'page_type': finding.get('page_type'),
            'screenshot_path': finding.get('screenshot_path')
        }
        deduplicator.process_finding(finding, url, metadata)

    deduplicated = deduplicator.get_deduplicated_findings()
    stats = deduplicator.get_statistics()

    logger.info(f"Deduplication complete:")
    logger.info(f"  Input: {len(findings)} findings")
    logger.info(f"  Output: {len(deduplicated)} unique findings")
    logger.info(f"  Reduction: {stats['deduplication_ratio']}")

    return deduplicated, stats
